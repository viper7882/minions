from minions.minion import Minion
from minions.minions import Minions
from minions.clients.ollama import OllamaClient
from minions.clients.openai import OpenAIClient
from minions.clients.anthropic import AnthropicClient
from minions.clients.together import TogetherClient
import time
import argparse
import fitz  # PyMuPDF for PDF handling
import json
from pydantic import BaseModel, Field

def extract_text_from_file(file_path):
    """Extract text from a PDF or TXT file."""
    try:
        if file_path.lower().endswith('.pdf'):
            # Handle PDF file
            doc = fitz.open(file_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text
        elif file_path.lower().endswith('.txt'):
            # Handle TXT file
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            raise ValueError("Unsupported file format. Only PDF and TXT files are supported.")
    except Exception as e:
        print(f"Error reading file: {str(e)}")
        return ""

def load_default_medical_context():
    try:
        with open("data/test_medical.txt", "r") as f:
            return f.read()
    except FileNotFoundError:
        print("Default medical context file not found!")
        return ""

class JobOutput(BaseModel):
  explanation: str
  citation: str | None
  answer: str | None

def format_usage(usage, model_name):
    total_tokens = usage.prompt_tokens + usage.completion_tokens
    return (
        f"\n{model_name} Usage Statistics:\n"
        f"  Prompt Tokens: {usage.prompt_tokens}\n"
        f"  Completion Tokens: {usage.completion_tokens}\n"
        f"  Total Tokens: {total_tokens}\n"
    )

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Run minion with specified provider')
    parser.add_argument('--provider', type=str, choices=['openai', 'anthropic', 'together'], default='openai',
                      help='The provider to use as remote client (default: openai)')
    parser.add_argument('--remote-model', type=str, default='gpt-4o-mini',
                      help='The remote model to use (default: gpt-4o-mini)')
    parser.add_argument('--local-model', type=str, default='llama3.2',
                      help='The local model to use (default: llama3.2)')
    parser.add_argument('--protocol', type=str, choices=['minion', 'minions'], default='minion',
                      help='The protocol to use (default: minion)')
    parser.add_argument('--file', type=str, default='/Users/biderman/Downloads/amazon_report.pdf',
                      help='Path to a PDF or TXT file to process (default: /Users/biderman/Downloads/amazon_report.pdf)')
    parser.add_argument('--doc-metadata', type=str, default='amazon 10k report',
                      help='Metadata describing the document (default: amazon 10k report)')
    parser.add_argument('--task', type=str, 
                      default="How did Amazon's operating income change from 2018 to 2019?",
                      help='The task or question to be answered')
    args = parser.parse_args()

    # Default parameters
    local_model_name = args.local_model
    local_temperature = 0.0
    local_max_tokens = 4096
    
    # Provider-specific parameters
    if args.provider == 'anthropic':
        remote_model_name = "claude-3-sonnet-20240229"
    elif args.provider == 'together':
        remote_model_name = "meta-llama/Llama-3.3-70B-Instruct-Turbo"
    else:  # openai
        remote_model_name = args.remote_model
    
    remote_temperature = 0.2
    remote_max_tokens = 2048
    
    # Load context from file
    context = extract_text_from_file(args.file)
    if not context:
        print("Error: Could not extract text from the specified file")
        return
    print("Truncating context to 70000 characters")
    context = context[10000:80000]
    
    print("Initializing clients...")
    setup_start_time = time.time()

    if args.protocol == "minions":
        # the local worker operates on chunks of data
        num_ctx = 4096
        structured_output_schema = JobOutput
        async_mode = True
    elif args.protocol == "minion":
        structured_output_schema = None
        async_mode = False
        # For Minion protocol, estimate tokens based on context length (4 chars â‰ˆ 1 token)
        # Add 4000 to account for the conversation history
        estimated_tokens = int(len(context) / 4 + 4000) if context else 4096
        # Round up to nearest power of 2 from predefined list
        num_ctx_values = [2048, 4096, 8192, 16384, 32768, 65536, 131072]
        # Find the smallest value that is >= estimated tokens
        num_ctx = min([x for x in num_ctx_values if x >= estimated_tokens], default=131072)
        print(f"Estimated tokens: {estimated_tokens}")
        print(f"Using context window: {num_ctx}")
    
    # Initialize the local client
    print(f"Initializing local client with model: {local_model_name}")
    local_client = OllamaClient(
        model_name=local_model_name,
        temperature=local_temperature,
        max_tokens=local_max_tokens,
        num_ctx=num_ctx,
        structured_output_schema=structured_output_schema,
        use_async=async_mode
    )
    
    # Initialize the remote client based on provider
    print(f"Initializing remote client with model: {remote_model_name}")
    if args.provider == 'anthropic':
        remote_client = AnthropicClient(
            model_name=remote_model_name,
            temperature=remote_temperature,
            max_tokens=remote_max_tokens
        )
    elif args.provider == 'together':
        remote_client = TogetherClient(
            model_name=remote_model_name,
            temperature=remote_temperature,
            max_tokens=remote_max_tokens
        )
    else:  # openai
        remote_client = OpenAIClient(
            model_name=remote_model_name,
            temperature=remote_temperature,
            max_tokens=remote_max_tokens
        )
    
    # Instantiate the protocol object with the clients
    print(f"Initializing {args.protocol} with clients")
    if args.protocol == 'minions':
        protocol = Minions(local_client, remote_client)
    else:  # minion
        protocol = Minion(local_client, remote_client)
    
    setup_time = time.time() - setup_start_time
    print(f"Setup completed in {setup_time:.2f} seconds")
    
    print("\nSolving task...")
    execution_start_time = time.time()
    
    # Execute the protocol
    output = protocol(
        task=args.task,
        doc_metadata=args.doc_metadata, # only for minions, to be removed soon
        context=[context],  # Use the extracted text from file as context
        max_rounds=2
    )
    
    execution_time = time.time() - execution_start_time
    total_time = setup_time + execution_time
    
    # Print results
    print("\n" + "="*50)
    print("Results:")
    print("="*50)
    print(f"\nFinal Answer:\n{output['final_answer']}")
    
    print("\nTiming Information:")
    print(f"Setup Time: {setup_time:.2f}s ({(setup_time/total_time*100):.1f}% of total)")
    print(f"Execution Time: {execution_time:.2f}s ({(execution_time/total_time*100):.1f}% of total)")
    
    # Print usage information if available
    if "local_usage" in output and "remote_usage" in output:
        print("\nUsage Information:")
        print(format_usage(output["local_usage"], local_model_name))
        print(format_usage(output["remote_usage"], remote_model_name))

    # Print protocol-specific information
    if args.protocol == 'minion':
        pass
        # print("\nSupervisor Messages:")
        # for msg in output["supervisor_messages"]:
        #     print(f"\n{msg['role'].capitalize()}: {msg['content']}")
        
        # print("\nWorker Messages:")
        # for msg in output["worker_messages"]:
        #     print(f"\n{msg['role'].capitalize()}: {msg['content']}")
    elif "meta" in output:  # minions protocol
        print("\nMeta Information:")
        for round_idx, round_meta in enumerate(output["meta"]):
            print(f"\nRound {round_idx + 1}:")
            if "local" in round_meta:
                print(f"Local jobs: {len(round_meta['local']['jobs'])}")
            if "remote" in round_meta:
                print(f"Remote messages: {len(round_meta['remote']['messages'])}")

    # Convert output to JSON-serializable format
    if "local_usage" in output and "remote_usage" in output:
        # Convert Usage objects to dictionaries
        output["local_usage"] = output["local_usage"].to_dict()
        output["remote_usage"] = output["remote_usage"].to_dict()

    # save the output to a file
    with open("/Users/biderman/Dropbox/Stanford/Dan/minions/test_output.json", "w") as f:
        json.dump(output, f)

if __name__ == "__main__":
    main() 